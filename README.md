
# ğŸš€ Data Platform with Airflow, Spark, and PostgreSQL

## ğŸ§­ Objective

Build a modern **Data Platform** to **collect**, **process**, and **store** data from various sources.  
This project simulates how data engineers in financial or economic domains manage data workflows.

ğŸ”— **System integrates:**

- ğŸ—“ï¸ **Apache Airflow** â€“ for ETL orchestration and scheduling  
- âš¡ **Apache Spark** â€“ for scalable and distributed data processing  
- ğŸ—„ï¸ **PostgreSQL** â€“ for storing cleaned data and supporting analytics/BI tools (e.g., Power BI)

---

## ğŸ—ï¸ System Architecture

```plaintext
ğŸ“‚ Data Sources (CSV / JSON / API / Streaming)
           â”‚
           â–¼
ğŸ“‹ Apache Airflow (DAG Scheduler)
           â”‚
           â–¼
âš™ï¸ Apache Spark (ETL Processing)
           â”‚
           â–¼
ğŸ§® PostgreSQL (Data Warehouse)
```

---

## ğŸ“Š Datasets

ğŸ“¥ Download from: [Kaggle - Transactions Fraud Datasets](https://www.kaggle.com/datasets/computingvictor/transactions-fraud-datasets)

ğŸ“ The archive contains:
- 3 CSV files  
- 2 JSON files  

### ğŸ’³ `transactions_data.csv`
- ğŸ’° Transaction records: amount, timestamp, merchant  
- ğŸ” Useful for spending pattern analysis & fraud detection

### ğŸ§¾ `cards_data.csv`
- ğŸ’³ Credit/debit card metadata  
- ğŸ”— Links with users via `card_id`  
- ğŸ“Š Helps profile customer financial behavior

### ğŸª `mcc_codes.json`
- ğŸ·ï¸ Business classification codes  
- ğŸ§  Enables categorizing transactions by industry

### ğŸš¨ `train_fraud_labels.json`
- âš–ï¸ Fraud / Legitimate labels  
- ğŸ§  Suitable for training supervised ML models

### ğŸ‘¤ `users_data.csv`
- ğŸ§¬ Demographic & account details  
- ğŸ¯ Enables segmentation and personalization

ğŸ“‚ After downloading and extracting, place the files in the `data/` folder.  
ğŸ“¦ `load/` and `staging/` folders will be **auto-generated** after running the ETL DAG.

ğŸ–¼ï¸ Example:

![Dataset](screenshots/datasetPlacing.png)

---

## ğŸ› ï¸ System Setup

âš ï¸ **NOTE for WSL Users:**  
Do **NOT** place this project in the `/mnt/` folder.  
WSL **mount volumes** do not allow writing files, which will break Airflow and Spark execution.

---

### ğŸ³ 1. Start Docker Containers

```bash
docker-compose up -d
```

This spins up:
- ğŸ—„ï¸ PostgreSQL Database  
- ğŸ“‹ Airflow Scheduler  
- ğŸŒ Airflow Webserver  
- âš™ï¸ Spark Master + Worker  

---

### ğŸŒ 2. Access Airflow UI

ğŸ“ Open your browser and go to:
```bash
localhost:8082
```

Youâ€™ll see the Apache Airflow interface:

![AirflowUI](screenshots/airflowUI.png)

---

ğŸ“‚ In this project directory:
- `dags/` â€“ contains the DAG for ETL  
- `main_pipeline.py` â€“ orchestrates task flow  
- `spark-jobs/` â€“ holds Spark transformation scripts  

âœ… After successful execution:
- `data/staging/` â€“ intermediate cleaned data  
- `data/load/` â€“ final transformed data ready for analytics

